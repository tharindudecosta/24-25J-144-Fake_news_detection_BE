{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f960958",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ef0fd13",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#python -m ipykernel install --user --name=fakeNewsDetectionEnv --display-name \"Python (fakeNewsDetectionEnv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T03:35:23.691126Z",
     "iopub.status.busy": "2024-11-15T03:35:23.690511Z",
     "iopub.status.idle": "2024-11-15T03:35:28.345238Z",
     "shell.execute_reply": "2024-11-15T03:35:28.344204Z",
     "shell.execute_reply.started": "2024-11-15T03:35:23.691092Z"
    },
    "id": "db7e7a3e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: torch-xla in /usr/local/lib/python3.10/site-packages (2.4.0+libtpu)\n",
      "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/site-packages (from torch-xla) (0.10)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from torch-xla) (6.0.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from torch-xla) (2.1.0)\n",
      "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla) (1.8.0)\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla) (4.1.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.0.1)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.35.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.34.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.2.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.6.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.4.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (5.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.1.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "\n",
    "# pip install torch torchvision torchaudio\n",
    "# Install PyTorch XLA for TPUs (usually needed on Kaggle or custom setups)\n",
    "!pip install torch-xla -f https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T03:37:52.942608Z",
     "iopub.status.busy": "2024-11-15T03:37:52.941715Z",
     "iopub.status.idle": "2024-11-15T03:37:53.009066Z",
     "shell.execute_reply": "2024-11-15T03:37:53.008107Z",
     "shell.execute_reply.started": "2024-11-15T03:37:52.942554Z"
    },
    "id": "W5jYVS9Tqa0K",
    "outputId": "5720e18c-7237-4d78-ca0c-652e2786c403",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T03:37:26.358526Z",
     "iopub.status.busy": "2024-11-15T03:37:26.357766Z",
     "iopub.status.idle": "2024-11-15T03:37:49.448156Z",
     "shell.execute_reply": "2024-11-15T03:37:49.447348Z",
     "shell.execute_reply.started": "2024-11-15T03:37:26.358493Z"
    },
    "id": "6O7ujhdTqhWM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()  # Use this after every epoch or large memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T03:37:55.478502Z",
     "iopub.status.busy": "2024-11-15T03:37:55.477793Z",
     "iopub.status.idle": "2024-11-15T03:37:55.481674Z",
     "shell.execute_reply": "2024-11-15T03:37:55.480941Z",
     "shell.execute_reply.started": "2024-11-15T03:37:55.478465Z"
    },
    "id": "ORiFgZRZnzYN",
    "outputId": "5349f019-5243-4dde-b32c-91e2121c7a0d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:16.561211Z",
     "iopub.status.busy": "2024-11-15T16:23:16.560801Z",
     "iopub.status.idle": "2024-11-15T16:23:16.565138Z",
     "shell.execute_reply": "2024-11-15T16:23:16.564297Z",
     "shell.execute_reply.started": "2024-11-15T16:23:16.561180Z"
    },
    "id": "46fc4c70",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:17.745630Z",
     "iopub.status.busy": "2024-11-15T16:23:17.745296Z",
     "iopub.status.idle": "2024-11-15T16:23:18.845061Z",
     "shell.execute_reply": "2024-11-15T16:23:18.843887Z",
     "shell.execute_reply.started": "2024-11-15T16:23:17.745605Z"
    },
    "id": "ffb2eefe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gossip_fake = pd.read_csv('/kaggle/input/fakenewsdataset/FakeNewsNet/dataset/gossipcop_fake.csv')\n",
    "\n",
    "gossip_real = pd.read_csv('/kaggle/input/fakenewsdataset/FakeNewsNet/dataset/gossipcop_real.csv')\n",
    "\n",
    "politifact_fake = pd.read_csv('/kaggle/input/fakenewsdataset/FakeNewsNet/dataset/politifact_fake.csv')\n",
    "\n",
    "politifact_real = pd.read_csv('/kaggle/input/fakenewsdataset/FakeNewsNet/dataset/politifact_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:18.846692Z",
     "iopub.status.busy": "2024-11-15T16:23:18.846409Z",
     "iopub.status.idle": "2024-11-15T16:23:18.852452Z",
     "shell.execute_reply": "2024-11-15T16:23:18.851599Z",
     "shell.execute_reply.started": "2024-11-15T16:23:18.846659Z"
    },
    "id": "c0b2ec33",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gossip_fake['label'] = 1\n",
    "\n",
    "gossip_real['label'] = 0\n",
    "\n",
    "politifact_fake['label'] = 1\n",
    "\n",
    "politifact_real['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:19.895770Z",
     "iopub.status.busy": "2024-11-15T16:23:19.894910Z",
     "iopub.status.idle": "2024-11-15T16:23:19.904517Z",
     "shell.execute_reply": "2024-11-15T16:23:19.903674Z",
     "shell.execute_reply.started": "2024-11-15T16:23:19.895738Z"
    },
    "id": "5b26cd21",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine fake and real news for both domains\n",
    "\n",
    "gossip = pd.concat([gossip_fake[['title', 'label']], gossip_real[['title', 'label']]])\n",
    "\n",
    "politifact = pd.concat([politifact_fake[['title', 'label']], politifact_real[['title', 'label']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:21.144177Z",
     "iopub.status.busy": "2024-11-15T16:23:21.143401Z",
     "iopub.status.idle": "2024-11-15T16:23:21.152965Z",
     "shell.execute_reply": "2024-11-15T16:23:21.152044Z",
     "shell.execute_reply.started": "2024-11-15T16:23:21.144143Z"
    },
    "id": "bd27925f",
    "outputId": "c5aff2af-4daf-4639-a6ed-121d225c2037",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Miley Cyrus and Liam Hemsworth secretly ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paris Jackson &amp; Cara Delevingne Enjoy Night Ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celebrities Join Tax March in Protest of Donal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cindy Crawford's daughter Kaia Gerber wears a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full List of 2018 Oscar Nominations – Variety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16812</th>\n",
       "      <td>2017 Hollywood Film Awards: The Complete List ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16813</th>\n",
       "      <td>Jada Pinkett Smith explains why son Jaden move...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16814</th>\n",
       "      <td>Tinsley Mortimer Reacts to Luann de Lesseps' R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16815</th>\n",
       "      <td>Prince Harry Carries on Princess Diana’s Legac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16816</th>\n",
       "      <td>Kylie Jenner is actually terrified of butterflies</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  label\n",
       "0      Did Miley Cyrus and Liam Hemsworth secretly ge...      1\n",
       "1      Paris Jackson & Cara Delevingne Enjoy Night Ou...      1\n",
       "2      Celebrities Join Tax March in Protest of Donal...      1\n",
       "3      Cindy Crawford's daughter Kaia Gerber wears a ...      1\n",
       "4          Full List of 2018 Oscar Nominations – Variety      1\n",
       "...                                                  ...    ...\n",
       "16812  2017 Hollywood Film Awards: The Complete List ...      0\n",
       "16813  Jada Pinkett Smith explains why son Jaden move...      0\n",
       "16814  Tinsley Mortimer Reacts to Luann de Lesseps' R...      0\n",
       "16815  Prince Harry Carries on Princess Diana’s Legac...      0\n",
       "16816  Kylie Jenner is actually terrified of butterflies      0\n",
       "\n",
       "[22140 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gossip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:25.775820Z",
     "iopub.status.busy": "2024-11-15T16:23:25.775057Z",
     "iopub.status.idle": "2024-11-15T16:23:25.780981Z",
     "shell.execute_reply": "2024-11-15T16:23:25.780186Z",
     "shell.execute_reply.started": "2024-11-15T16:23:25.775789Z"
    },
    "id": "7f401345",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "news_df = pd.concat([gossip, politifact], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:27.344813Z",
     "iopub.status.busy": "2024-11-15T16:23:27.344224Z",
     "iopub.status.idle": "2024-11-15T16:23:27.351793Z",
     "shell.execute_reply": "2024-11-15T16:23:27.350957Z",
     "shell.execute_reply.started": "2024-11-15T16:23:27.344785Z"
    },
    "id": "d5b8bf87",
    "outputId": "3d735142-21e6-4940-82c8-f5e836bb4046",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Miley Cyrus and Liam Hemsworth secretly ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paris Jackson &amp; Cara Delevingne Enjoy Night Ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celebrities Join Tax March in Protest of Donal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cindy Crawford's daughter Kaia Gerber wears a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full List of 2018 Oscar Nominations – Variety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0  Did Miley Cyrus and Liam Hemsworth secretly ge...      1\n",
       "1  Paris Jackson & Cara Delevingne Enjoy Night Ou...      1\n",
       "2  Celebrities Join Tax March in Protest of Donal...      1\n",
       "3  Cindy Crawford's daughter Kaia Gerber wears a ...      1\n",
       "4      Full List of 2018 Oscar Nominations – Variety      1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:23:46.244339Z",
     "iopub.status.busy": "2024-11-15T16:23:46.243964Z",
     "iopub.status.idle": "2024-11-15T16:23:46.409813Z",
     "shell.execute_reply": "2024-11-15T16:23:46.408963Z",
     "shell.execute_reply.started": "2024-11-15T16:23:46.244309Z"
    },
    "id": "99ff3476",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove special characters and digits, and convert text to lowercase\n",
    "\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# Apply cleaning to all articles\n",
    "\n",
    "news_df['title'] = news_df['title'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "579a27a1",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:36:30.669864Z",
     "iopub.status.busy": "2024-11-14T01:36:30.669122Z",
     "iopub.status.idle": "2024-11-14T01:37:18.724152Z",
     "shell.execute_reply": "2024-11-14T01:37:18.723391Z",
     "shell.execute_reply.started": "2024-11-14T01:36:30.669830Z"
    },
    "id": "a7581586",
    "outputId": "fe34a346-6db9-4e4a-9f90-f313a4e6ef0a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "W1114 05:31:52.973000 140635694196416 torch/_inductor/compile_worker/subproc_pool.py:126] SubprocPool unclean exit\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "# Apply the tokenizer to the text column\n",
    "\n",
    "news_df['input_ids'] = news_df['title'].apply(lambda x: tokenizer(x, padding='max_length', truncation=True, max_length=512)['input_ids'])\n",
    "\n",
    "news_df['attention_mask'] = news_df['title'].apply(lambda x: tokenizer(x, padding='max_length', truncation=True, max_length=512)['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:37:18.726217Z",
     "iopub.status.busy": "2024-11-14T01:37:18.725680Z",
     "iopub.status.idle": "2024-11-14T01:37:18.740862Z",
     "shell.execute_reply": "2024-11-14T01:37:18.740256Z",
     "shell.execute_reply.started": "2024-11-14T01:37:18.726182Z"
    },
    "id": "6f4919ed",
    "outputId": "4d331d80-bf06-49f1-c398-981e2da4fb65",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did miley cyrus and liam hemsworth secretly ge...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2106, 3542, 2100, 16123, 1998, 8230, 196...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paris jackson   cara delevingne enjoy night ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 3000, 4027, 14418, 3972, 23559, 2638, 59...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>celebrities join tax march in protest of donal...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 12330, 3693, 4171, 2233, 1999, 6186, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cindy crawford s daughter kaia gerber wears a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 15837, 10554, 1055, 2684, 11928, 2050, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full list of      oscar nominations   variety</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2440, 2862, 1997, 7436, 9930, 3528, 102,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label  \\\n",
       "0  did miley cyrus and liam hemsworth secretly ge...      1   \n",
       "1  paris jackson   cara delevingne enjoy night ou...      1   \n",
       "2  celebrities join tax march in protest of donal...      1   \n",
       "3  cindy crawford s daughter kaia gerber wears a ...      1   \n",
       "4      full list of      oscar nominations   variety      1   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 2106, 3542, 2100, 16123, 1998, 8230, 196...   \n",
       "1  [101, 3000, 4027, 14418, 3972, 23559, 2638, 59...   \n",
       "2  [101, 12330, 3693, 4171, 2233, 1999, 6186, 199...   \n",
       "3  [101, 15837, 10554, 1055, 2684, 11928, 2050, 1...   \n",
       "4  [101, 2440, 2862, 1997, 7436, 9930, 3528, 102,...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:37:18.741984Z",
     "iopub.status.busy": "2024-11-14T01:37:18.741718Z",
     "iopub.status.idle": "2024-11-14T01:37:18.752940Z",
     "shell.execute_reply": "2024-11-14T01:37:18.752331Z",
     "shell.execute_reply.started": "2024-11-14T01:37:18.741957Z"
    },
    "id": "650b4c84",
    "outputId": "db0146dd-8371-4bf4-9f46-e1c0fcfc7504",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23196, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:37:18.754657Z",
     "iopub.status.busy": "2024-11-14T01:37:18.754311Z",
     "iopub.status.idle": "2024-11-14T01:37:20.704976Z",
     "shell.execute_reply": "2024-11-14T01:37:20.704086Z",
     "shell.execute_reply.started": "2024-11-14T01:37:18.754630Z"
    },
    "id": "643899e8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(news_df[['input_ids', 'attention_mask']], news_df['label'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:37:20.706615Z",
     "iopub.status.busy": "2024-11-14T01:37:20.706143Z",
     "iopub.status.idle": "2024-11-14T01:37:25.155728Z",
     "shell.execute_reply": "2024-11-14T01:37:25.154921Z",
     "shell.execute_reply.started": "2024-11-14T01:37:20.706586Z"
    },
    "id": "1f16b079",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "\n",
    "train_inputs = torch.tensor([x for x in train_texts['input_ids']])\n",
    "\n",
    "train_masks = torch.tensor([x for x in train_texts['attention_mask']])\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values)\n",
    "\n",
    "\n",
    "\n",
    "val_inputs = torch.tensor([x for x in val_texts['input_ids']])\n",
    "\n",
    "val_masks = torch.tensor([x for x in val_texts['attention_mask']])\n",
    "\n",
    "val_labels = torch.tensor(val_labels.values)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T21:07:16.618423Z",
     "iopub.status.busy": "2024-11-13T21:07:16.617629Z",
     "iopub.status.idle": "2024-11-14T00:26:14.898031Z",
     "shell.execute_reply": "2024-11-14T00:26:14.897120Z",
     "shell.execute_reply.started": "2024-11-13T21:07:16.618391Z"
    },
    "id": "459e488c",
    "outputId": "5e0b8bad-08de-4474-f41a-321ed492796f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_13/1247459758.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch [1/3], Batch [0/5219], Loss: 0.6816, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [100/5219], Loss: 0.5717, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [200/5219], Loss: 0.7850, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [300/5219], Loss: 0.3562, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [400/5219], Loss: 0.2894, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [500/5219], Loss: 0.1384, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [600/5219], Loss: 0.4608, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [700/5219], Loss: 0.2784, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [800/5219], Loss: 1.5725, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [900/5219], Loss: 0.8742, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [1000/5219], Loss: 0.2002, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [1100/5219], Loss: 0.8140, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [1200/5219], Loss: 0.7809, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [1300/5219], Loss: 0.1786, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [1400/5219], Loss: 0.0647, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [1500/5219], Loss: 0.1186, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [1600/5219], Loss: 0.3998, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [1700/5219], Loss: 0.4715, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [1800/5219], Loss: 0.3524, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [1900/5219], Loss: 0.2015, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [2000/5219], Loss: 0.6356, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [2100/5219], Loss: 0.1354, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [2200/5219], Loss: 0.2257, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [2300/5219], Loss: 0.5394, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [2400/5219], Loss: 0.8956, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [2500/5219], Loss: 0.9460, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [2600/5219], Loss: 0.7932, Time: 0.77 seconds\n",
      "Epoch [1/3], Batch [2700/5219], Loss: 0.0874, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [2800/5219], Loss: 0.4224, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [2900/5219], Loss: 0.3158, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [3000/5219], Loss: 0.5947, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [3100/5219], Loss: 0.4579, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [3200/5219], Loss: 0.1124, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [3300/5219], Loss: 0.2048, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [3400/5219], Loss: 0.1163, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [3500/5219], Loss: 0.4183, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [3600/5219], Loss: 0.1380, Time: 0.78 seconds\n",
      "Epoch [1/3], Batch [3700/5219], Loss: 0.2771, Time: 0.78 seconds\n",
      "Epoch [1/3], Batch [3800/5219], Loss: 0.4090, Time: 0.79 seconds\n",
      "Epoch [1/3], Batch [3900/5219], Loss: 0.2704, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [4000/5219], Loss: 0.2343, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [4100/5219], Loss: 0.0502, Time: 0.79 seconds\n",
      "Epoch [1/3], Batch [4200/5219], Loss: 1.1896, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [4300/5219], Loss: 0.0913, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [4400/5219], Loss: 0.9588, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [4500/5219], Loss: 0.5385, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [4600/5219], Loss: 0.4993, Time: 0.76 seconds\n",
      "Epoch [1/3], Batch [4700/5219], Loss: 0.4139, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [4800/5219], Loss: 0.6505, Time: 0.80 seconds\n",
      "Epoch [1/3], Batch [4900/5219], Loss: 0.3766, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [5000/5219], Loss: 0.0987, Time: 0.77 seconds\n",
      "Epoch [1/3], Batch [5100/5219], Loss: 0.5879, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [5200/5219], Loss: 0.9641, Time: 0.75 seconds\n",
      "Epoch [1/3] completed in 3888.32 seconds.\n",
      "Validation loss: 0.3597, Validation accuracy: 0.8448\n",
      "Epoch 2/3\n",
      "Epoch [2/3], Batch [0/5219], Loss: 0.3005, Time: 0.68 seconds\n",
      "Epoch [2/3], Batch [100/5219], Loss: 0.0056, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [200/5219], Loss: 0.2790, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [300/5219], Loss: 0.3595, Time: 0.79 seconds\n",
      "Epoch [2/3], Batch [400/5219], Loss: 0.0121, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [500/5219], Loss: 0.2707, Time: 0.82 seconds\n",
      "Epoch [2/3], Batch [600/5219], Loss: 0.0268, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [700/5219], Loss: 0.6458, Time: 0.76 seconds\n",
      "Epoch [2/3], Batch [800/5219], Loss: 0.8384, Time: 0.78 seconds\n",
      "Epoch [2/3], Batch [900/5219], Loss: 0.0585, Time: 0.78 seconds\n",
      "Epoch [2/3], Batch [1000/5219], Loss: 0.7157, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1100/5219], Loss: 0.1151, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1200/5219], Loss: 0.2094, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [1300/5219], Loss: 0.0300, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1400/5219], Loss: 0.1720, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1500/5219], Loss: 0.6938, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [1600/5219], Loss: 1.3786, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [1700/5219], Loss: 0.1751, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1800/5219], Loss: 0.2149, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1900/5219], Loss: 0.1356, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [2000/5219], Loss: 0.1439, Time: 0.78 seconds\n",
      "Epoch [2/3], Batch [2100/5219], Loss: 0.0448, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [2200/5219], Loss: 0.3838, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [2300/5219], Loss: 0.1993, Time: 0.78 seconds\n",
      "Epoch [2/3], Batch [2400/5219], Loss: 0.0184, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [2500/5219], Loss: 0.0821, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [2600/5219], Loss: 0.0459, Time: 0.76 seconds\n",
      "Epoch [2/3], Batch [2700/5219], Loss: 0.0145, Time: 0.77 seconds\n",
      "Epoch [2/3], Batch [2800/5219], Loss: 0.2217, Time: 0.79 seconds\n",
      "Epoch [2/3], Batch [2900/5219], Loss: 0.6898, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [3000/5219], Loss: 0.0983, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [3100/5219], Loss: 0.0882, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [3200/5219], Loss: 1.0176, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [3300/5219], Loss: 0.0413, Time: 0.76 seconds\n",
      "Epoch [2/3], Batch [3400/5219], Loss: 0.0596, Time: 0.77 seconds\n",
      "Epoch [2/3], Batch [3500/5219], Loss: 0.0868, Time: 0.80 seconds\n",
      "Epoch [2/3], Batch [3600/5219], Loss: 0.0262, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [3700/5219], Loss: 0.2417, Time: 0.76 seconds\n",
      "Epoch [2/3], Batch [3800/5219], Loss: 0.1301, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [3900/5219], Loss: 0.0755, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [4000/5219], Loss: 0.0419, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [4100/5219], Loss: 0.0214, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [4200/5219], Loss: 0.1684, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [4300/5219], Loss: 1.4675, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [4400/5219], Loss: 0.0725, Time: 0.76 seconds\n",
      "Epoch [2/3], Batch [4500/5219], Loss: 0.0251, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [4600/5219], Loss: 0.0925, Time: 0.78 seconds\n",
      "Epoch [2/3], Batch [4700/5219], Loss: 0.0646, Time: 0.78 seconds\n",
      "Epoch [2/3], Batch [4800/5219], Loss: 0.1384, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [4900/5219], Loss: 0.0422, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [5000/5219], Loss: 0.2022, Time: 0.77 seconds\n",
      "Epoch [2/3], Batch [5100/5219], Loss: 0.1423, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [5200/5219], Loss: 0.0831, Time: 0.74 seconds\n",
      "Epoch [2/3] completed in 3944.67 seconds.\n",
      "Validation loss: 0.3542, Validation accuracy: 0.8578\n",
      "Epoch 3/3\n",
      "Epoch [3/3], Batch [0/5219], Loss: 0.1670, Time: 0.71 seconds\n",
      "Epoch [3/3], Batch [100/5219], Loss: 0.4403, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [200/5219], Loss: 0.0347, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [300/5219], Loss: 0.0214, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [400/5219], Loss: 0.1390, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [500/5219], Loss: 0.0114, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [600/5219], Loss: 0.1208, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [700/5219], Loss: 0.0189, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [800/5219], Loss: 0.0417, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [900/5219], Loss: 0.2022, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [1000/5219], Loss: 0.0250, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [1100/5219], Loss: 0.3602, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [1200/5219], Loss: 0.0335, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [1300/5219], Loss: 0.1402, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [1400/5219], Loss: 0.0237, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [1500/5219], Loss: 0.5726, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [1600/5219], Loss: 0.0535, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [1700/5219], Loss: 0.0104, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [1800/5219], Loss: 0.0113, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [1900/5219], Loss: 1.0856, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [2000/5219], Loss: 0.3535, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [2100/5219], Loss: 0.1490, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [2200/5219], Loss: 0.1639, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [2300/5219], Loss: 0.0642, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [2400/5219], Loss: 0.0297, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [2500/5219], Loss: 0.0392, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [2600/5219], Loss: 0.0506, Time: 0.78 seconds\n",
      "Epoch [3/3], Batch [2700/5219], Loss: 0.1273, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [2800/5219], Loss: 0.1109, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [2900/5219], Loss: 0.0483, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [3000/5219], Loss: 0.1372, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [3100/5219], Loss: 0.0010, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [3200/5219], Loss: 0.0155, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [3300/5219], Loss: 0.0169, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [3400/5219], Loss: 0.0180, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [3500/5219], Loss: 0.0268, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [3600/5219], Loss: 0.0122, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [3700/5219], Loss: 0.0124, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [3800/5219], Loss: 0.0201, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [3900/5219], Loss: 0.0427, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [4000/5219], Loss: 0.0575, Time: 0.78 seconds\n",
      "Epoch [3/3], Batch [4100/5219], Loss: 0.0186, Time: 0.82 seconds\n",
      "Epoch [3/3], Batch [4200/5219], Loss: 0.1338, Time: 0.78 seconds\n",
      "Epoch [3/3], Batch [4300/5219], Loss: 0.0024, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4400/5219], Loss: 0.0125, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [4500/5219], Loss: 0.0422, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4600/5219], Loss: 0.0098, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4700/5219], Loss: 0.0044, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [4800/5219], Loss: 0.0394, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [4900/5219], Loss: 0.0021, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [5000/5219], Loss: 0.0057, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [5100/5219], Loss: 0.0088, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [5200/5219], Loss: 0.0899, Time: 0.76 seconds\n",
      "Epoch [3/3] completed in 3978.59 seconds.\n",
      "Validation loss: 0.4653, Validation accuracy: 0.8573\n",
      "Saving the fine-tuned model...\n",
      "Model and Tokenizer saved to './fake_news_model'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.cuda.amp import GradScaler  # Only use GradScaler without autocast for TPU\n",
    "\n",
    "# Define the main training function\n",
    "def train_tpu():\n",
    "    # Setup device (TPU core)\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    # Load BERT model and move it to TPU device\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    epochs = 3\n",
    "    \n",
    "    # DataLoader for training and validation\n",
    "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=4)\n",
    "    val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=4)\n",
    "    \n",
    "    # Total training steps and scheduler\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    scaler = GradScaler()  # Mixed precision\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()  # Record start time for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
    "        for step, batch in enumerate(para_loader.per_device_loader(device)):\n",
    "            start_batch_time = time.time()  # Record start time for the batch\n",
    "            batch_input_ids, batch_input_mask, batch_labels = (\n",
    "                batch[0].to(device),\n",
    "                batch[1].to(device),\n",
    "                batch[2].to(device),\n",
    "            )\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass without autocast\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass with scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log the current batch time\n",
    "            batch_time = time.time() - start_batch_time\n",
    "            if step % 100 == 0:  # Log every 100 steps\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{step}/{len(train_dataloader)}], Loss: {loss.item():.4f}, Time: {batch_time:.2f} seconds\")\n",
    "\n",
    "        # Log the epoch time\n",
    "        epoch_time = time.time() - start_epoch_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_accuracy = 0\n",
    "        para_loader = pl.ParallelLoader(val_dataloader, [device])\n",
    "        for batch in para_loader.per_device_loader(device):\n",
    "            batch_input_ids, batch_input_mask, batch_labels = (\n",
    "                batch[0].to(device),\n",
    "                batch[1].to(device),\n",
    "                batch[2].to(device),\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                # Predictions for accuracy\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                eval_accuracy += accuracy_score(batch_labels.cpu(), preds.cpu())\n",
    "\n",
    "        # Average validation loss and accuracy\n",
    "        avg_val_loss = eval_loss / len(val_dataloader)\n",
    "        avg_val_accuracy = eval_accuracy / len(val_dataloader)\n",
    "        xm.master_print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "# Run training and get the trained model\n",
    "trained_model = train_tpu()\n",
    "trained_model.to(\"cpu\")\n",
    "\n",
    "# After training is finished, save the model\n",
    "print(\"Saving the fine-tuned model...\")\n",
    "trained_model.save_pretrained('./fake_news_model')\n",
    "tokenizer.save_pretrained('./fake_news_tokenizer')\n",
    "print(\"Model and Tokenizer saved to './fake_news_model'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T05:05:55.804465Z",
     "iopub.status.busy": "2024-11-14T05:05:55.803438Z",
     "iopub.status.idle": "2024-11-14T05:06:16.866977Z",
     "shell.execute_reply": "2024-11-14T05:06:16.865773Z",
     "shell.execute_reply.started": "2024-11-14T05:05:55.804427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer have been zipped.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip the model and tokenizer directories\n",
    "shutil.make_archive(\"/kaggle/working/fake_news_model\", 'zip', \"./fake_news_model\")\n",
    "shutil.make_archive(\"/kaggle/working/fake_news_tokenizer\", 'zip', \"./fake_news_tokenizer\")\n",
    "\n",
    "print(\"Model and tokenizer have been zipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T00:30:27.134441Z",
     "iopub.status.busy": "2024-11-14T00:30:27.133567Z",
     "iopub.status.idle": "2024-11-14T00:30:27.804796Z",
     "shell.execute_reply": "2024-11-14T00:30:27.803545Z",
     "shell.execute_reply.started": "2024-11-14T00:30:27.134398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(trained_model.state_dict(), '/kaggle/working/model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T05:06:48.613765Z",
     "iopub.status.busy": "2024-11-14T05:06:48.613059Z",
     "iopub.status.idle": "2024-11-14T05:06:48.620473Z",
     "shell.execute_reply": "2024-11-14T05:06:48.619068Z",
     "shell.execute_reply.started": "2024-11-14T05:06:48.613732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='fake_news_model.zip' target='_blank'>fake_news_model.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/fake_news_model.zip"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'fake_news_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T00:50:30.627976Z",
     "iopub.status.busy": "2024-11-14T00:50:30.627178Z",
     "iopub.status.idle": "2024-11-14T00:50:30.633316Z",
     "shell.execute_reply": "2024-11-14T00:50:30.632474Z",
     "shell.execute_reply.started": "2024-11-14T00:50:30.627939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='model_state.pth' target='_blank'>model_state.pth</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/model_state.pth"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(r'model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:16:26.775436Z",
     "iopub.status.busy": "2024-11-14T01:16:26.774480Z",
     "iopub.status.idle": "2024-11-14T01:16:41.006750Z",
     "shell.execute_reply": "2024-11-14T01:16:41.005725Z",
     "shell.execute_reply.started": "2024-11-14T01:16:26.775399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5200\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.86      0.21      0.33        29\n",
      "     Class 1       0.47      0.95      0.62        21\n",
      "\n",
      "    accuracy                           0.52        50\n",
      "   macro avg       0.66      0.58      0.48        50\n",
      "weighted avg       0.69      0.52      0.46        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load test set\n",
    "test_df = pd.read_csv('/kaggle/input/test-set/train.csv')\n",
    "\n",
    "# Extract the top 50 rows\n",
    "test_df = test_df.head(50)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('./fake_news_tokenizer')\n",
    "model = BertForSequenceClassification.from_pretrained('./fake_news_model')\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the test data\n",
    "inputs = tokenizer(list(test_df['text']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_df['label'], predictions)\n",
    "report = classification_report(test_df['label'], predictions, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:20:27.268118Z",
     "iopub.status.busy": "2024-11-14T01:20:27.267715Z",
     "iopub.status.idle": "2024-11-14T01:20:27.272879Z",
     "shell.execute_reply": "2024-11-14T01:20:27.271900Z",
     "shell.execute_reply.started": "2024-11-14T01:20:27.268087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:20:59.381806Z",
     "iopub.status.busy": "2024-11-14T01:20:59.381378Z",
     "iopub.status.idle": "2024-11-14T01:20:59.388624Z",
     "shell.execute_reply": "2024-11-14T01:20:59.387808Z",
     "shell.execute_reply.started": "2024-11-14T01:20:59.381774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training data:\n",
      "label\n",
      "0    17441\n",
      "1     5755\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataset is in a DataFrame named 'train_data' and the label column is named 'label'\n",
    "label_counts = news_df['label'].value_counts()\n",
    "print(\"Class distribution in training data:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T01:45:28.704987Z",
     "iopub.status.busy": "2024-11-14T01:45:28.704594Z",
     "iopub.status.idle": "2024-11-14T05:01:08.327676Z",
     "shell.execute_reply": "2024-11-14T05:01:08.326341Z",
     "shell.execute_reply.started": "2024-11-14T01:45:28.704955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_13/2767510910.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch [1/3], Batch [0/5219], Loss: 0.8029, Time: 39.38 seconds\n",
      "Epoch [1/3], Batch [100/5219], Loss: 0.6714, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [200/5219], Loss: 0.6456, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [300/5219], Loss: 0.8562, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [400/5219], Loss: 0.8011, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [500/5219], Loss: 0.5158, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [600/5219], Loss: 0.9125, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [700/5219], Loss: 0.2648, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [800/5219], Loss: 0.7612, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [900/5219], Loss: 1.0252, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [1000/5219], Loss: 0.3916, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [1100/5219], Loss: 0.2998, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [1200/5219], Loss: 0.3034, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [1300/5219], Loss: 0.2480, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [1400/5219], Loss: 0.1953, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [1500/5219], Loss: 0.6336, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [1600/5219], Loss: 0.2570, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [1700/5219], Loss: 0.3983, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [1800/5219], Loss: 0.7376, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [1900/5219], Loss: 0.4096, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [2000/5219], Loss: 0.2832, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [2100/5219], Loss: 0.6015, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [2200/5219], Loss: 1.2832, Time: 0.75 seconds\n",
      "Epoch [1/3], Batch [2300/5219], Loss: 0.3435, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [2400/5219], Loss: 0.2778, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [2500/5219], Loss: 0.4399, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [2600/5219], Loss: 0.5116, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [2700/5219], Loss: 0.4746, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [2800/5219], Loss: 0.6127, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [2900/5219], Loss: 0.7189, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [3000/5219], Loss: 1.7518, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [3100/5219], Loss: 0.3921, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [3200/5219], Loss: 1.5649, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [3300/5219], Loss: 0.7780, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [3400/5219], Loss: 0.2763, Time: 0.73 seconds\n",
      "Epoch [1/3], Batch [3500/5219], Loss: 0.2700, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [3600/5219], Loss: 0.3786, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [3700/5219], Loss: 0.3530, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [3800/5219], Loss: 0.2767, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [3900/5219], Loss: 0.3493, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [4000/5219], Loss: 0.2296, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [4100/5219], Loss: 0.4677, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [4200/5219], Loss: 0.6626, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [4300/5219], Loss: 0.3431, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [4400/5219], Loss: 0.0801, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [4500/5219], Loss: 0.4243, Time: 0.70 seconds\n",
      "Epoch [1/3], Batch [4600/5219], Loss: 0.7806, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [4700/5219], Loss: 0.7769, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [4800/5219], Loss: 0.3682, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [4900/5219], Loss: 0.6120, Time: 0.72 seconds\n",
      "Epoch [1/3], Batch [5000/5219], Loss: 0.4576, Time: 0.74 seconds\n",
      "Epoch [1/3], Batch [5100/5219], Loss: 0.5182, Time: 0.71 seconds\n",
      "Epoch [1/3], Batch [5200/5219], Loss: 0.6621, Time: 0.72 seconds\n",
      "Epoch [1/3] completed in 3833.38 seconds.\n",
      "Validation loss: 0.4450, Validation accuracy: 0.8323\n",
      "Epoch 2/3\n",
      "Epoch [2/3], Batch [0/5219], Loss: 0.1314, Time: 0.66 seconds\n",
      "Epoch [2/3], Batch [100/5219], Loss: 0.0444, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [200/5219], Loss: 0.1463, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [300/5219], Loss: 0.4299, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [400/5219], Loss: 0.6747, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [500/5219], Loss: 0.1097, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [600/5219], Loss: 1.2831, Time: 0.70 seconds\n",
      "Epoch [2/3], Batch [700/5219], Loss: 1.3497, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [800/5219], Loss: 0.6192, Time: 0.70 seconds\n",
      "Epoch [2/3], Batch [900/5219], Loss: 0.0793, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [1000/5219], Loss: 0.2745, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [1100/5219], Loss: 0.0498, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [1200/5219], Loss: 0.1587, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1300/5219], Loss: 0.4706, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [1400/5219], Loss: 0.2079, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [1500/5219], Loss: 0.0464, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [1600/5219], Loss: 0.0673, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [1700/5219], Loss: 0.0737, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [1800/5219], Loss: 0.0560, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [1900/5219], Loss: 0.0559, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [2000/5219], Loss: 0.0972, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [2100/5219], Loss: 0.5727, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [2200/5219], Loss: 0.1241, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [2300/5219], Loss: 0.1796, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [2400/5219], Loss: 0.4694, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [2500/5219], Loss: 0.2581, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [2600/5219], Loss: 0.3071, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [2700/5219], Loss: 0.2087, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [2800/5219], Loss: 0.4244, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [2900/5219], Loss: 0.2432, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [3000/5219], Loss: 0.3525, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [3100/5219], Loss: 0.0714, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [3200/5219], Loss: 1.4747, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [3300/5219], Loss: 0.5552, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [3400/5219], Loss: 0.1046, Time: 0.80 seconds\n",
      "Epoch [2/3], Batch [3500/5219], Loss: 0.1293, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [3600/5219], Loss: 0.1305, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [3700/5219], Loss: 0.3299, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [3800/5219], Loss: 0.3046, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [3900/5219], Loss: 0.1648, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [4000/5219], Loss: 0.4578, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [4100/5219], Loss: 0.3164, Time: 0.70 seconds\n",
      "Epoch [2/3], Batch [4200/5219], Loss: 0.2642, Time: 0.73 seconds\n",
      "Epoch [2/3], Batch [4300/5219], Loss: 0.0766, Time: 0.71 seconds\n",
      "Epoch [2/3], Batch [4400/5219], Loss: 1.1567, Time: 0.72 seconds\n",
      "Epoch [2/3], Batch [4500/5219], Loss: 0.0884, Time: 0.70 seconds\n",
      "Epoch [2/3], Batch [4600/5219], Loss: 0.4413, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [4700/5219], Loss: 0.4188, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [4800/5219], Loss: 0.5142, Time: 0.81 seconds\n",
      "Epoch [2/3], Batch [4900/5219], Loss: 0.9425, Time: 0.74 seconds\n",
      "Epoch [2/3], Batch [5000/5219], Loss: 0.6219, Time: 0.76 seconds\n",
      "Epoch [2/3], Batch [5100/5219], Loss: 0.3047, Time: 0.75 seconds\n",
      "Epoch [2/3], Batch [5200/5219], Loss: 1.7681, Time: 0.72 seconds\n",
      "Epoch [2/3] completed in 3842.58 seconds.\n",
      "Validation loss: 0.4496, Validation accuracy: 0.8276\n",
      "Epoch 3/3\n",
      "Epoch [3/3], Batch [0/5219], Loss: 0.7205, Time: 0.72 seconds\n",
      "Epoch [3/3], Batch [100/5219], Loss: 0.0388, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [200/5219], Loss: 0.0727, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [300/5219], Loss: 0.0171, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [400/5219], Loss: 0.0898, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [500/5219], Loss: 0.0390, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [600/5219], Loss: 0.1552, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [700/5219], Loss: 0.7325, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [800/5219], Loss: 0.0506, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [900/5219], Loss: 0.0390, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [1000/5219], Loss: 0.0524, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [1100/5219], Loss: 0.2176, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [1200/5219], Loss: 0.0229, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [1300/5219], Loss: 0.0062, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [1400/5219], Loss: 0.6568, Time: 0.78 seconds\n",
      "Epoch [3/3], Batch [1500/5219], Loss: 0.0942, Time: 0.78 seconds\n",
      "Epoch [3/3], Batch [1600/5219], Loss: 0.4722, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [1700/5219], Loss: 0.2227, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [1800/5219], Loss: 0.0069, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [1900/5219], Loss: 0.0122, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [2000/5219], Loss: 0.0101, Time: 0.72 seconds\n",
      "Epoch [3/3], Batch [2100/5219], Loss: 0.1942, Time: 0.72 seconds\n",
      "Epoch [3/3], Batch [2200/5219], Loss: 0.2915, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [2300/5219], Loss: 0.2889, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [2400/5219], Loss: 0.0635, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [2500/5219], Loss: 0.1131, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [2600/5219], Loss: 0.0326, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [2700/5219], Loss: 0.0226, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [2800/5219], Loss: 0.1922, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [2900/5219], Loss: 0.1455, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [3000/5219], Loss: 0.0093, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [3100/5219], Loss: 0.1137, Time: 0.80 seconds\n",
      "Epoch [3/3], Batch [3200/5219], Loss: 0.3509, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [3300/5219], Loss: 0.2056, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [3400/5219], Loss: 0.3578, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [3500/5219], Loss: 0.1094, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [3600/5219], Loss: 0.0105, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [3700/5219], Loss: 0.0117, Time: 0.77 seconds\n",
      "Epoch [3/3], Batch [3800/5219], Loss: 0.0677, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [3900/5219], Loss: 0.0765, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4000/5219], Loss: 0.0439, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [4100/5219], Loss: 0.0118, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4200/5219], Loss: 0.0778, Time: 0.75 seconds\n",
      "Epoch [3/3], Batch [4300/5219], Loss: 0.4667, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [4400/5219], Loss: 0.0124, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4500/5219], Loss: 1.0800, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [4600/5219], Loss: 0.0300, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [4700/5219], Loss: 0.0742, Time: 0.73 seconds\n",
      "Epoch [3/3], Batch [4800/5219], Loss: 0.0266, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [4900/5219], Loss: 0.0114, Time: 0.79 seconds\n",
      "Epoch [3/3], Batch [5000/5219], Loss: 0.1027, Time: 0.76 seconds\n",
      "Epoch [3/3], Batch [5100/5219], Loss: 0.0585, Time: 0.74 seconds\n",
      "Epoch [3/3], Batch [5200/5219], Loss: 0.0274, Time: 0.74 seconds\n",
      "Epoch [3/3] completed in 3943.07 seconds.\n",
      "Validation loss: 0.5692, Validation accuracy: 0.8440\n",
      "Saving the fine-tuned model...\n",
      "Model and Tokenizer saved to './fake_news_model'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.cuda.amp import GradScaler  # Only use GradScaler without autocast for TPU\n",
    "\n",
    "# Define the main training function\n",
    "def train_tpu():\n",
    "    # Setup device (TPU core)\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    # Calculate weights for each class\n",
    "    class_weights = torch.tensor([1.0, 17441 / 5755], device=device)  # Modify this ratio based on your counts\n",
    "    \n",
    "    # Load BERT model and move it to TPU device\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    epochs = 3\n",
    "    \n",
    "    # DataLoader for training and validation\n",
    "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=4)\n",
    "    val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=4)\n",
    "    \n",
    "    # Total training steps and scheduler\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    scaler = GradScaler()  # Mixed precision\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()  # Record start time for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
    "        for step, batch in enumerate(para_loader.per_device_loader(device)):\n",
    "            start_batch_time = time.time()  # Record start time for the batch\n",
    "            batch_input_ids, batch_input_mask, batch_labels = (\n",
    "                batch[0].to(device),\n",
    "                batch[1].to(device),\n",
    "                batch[2].to(device),\n",
    "            )\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Apply class weights to the loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "            loss = loss_fct(outputs.logits, batch_labels)  # Use logits and labels directly for the loss calculation\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass with scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log the current batch time\n",
    "            batch_time = time.time() - start_batch_time\n",
    "            if step % 100 == 0:  # Log every 100 steps\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{step}/{len(train_dataloader)}], Loss: {loss.item():.4f}, Time: {batch_time:.2f} seconds\")\n",
    "\n",
    "        # Log the epoch time\n",
    "        epoch_time = time.time() - start_epoch_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_accuracy = 0\n",
    "        para_loader = pl.ParallelLoader(val_dataloader, [device])\n",
    "        for batch in para_loader.per_device_loader(device):\n",
    "            batch_input_ids, batch_input_mask, batch_labels = (\n",
    "                batch[0].to(device),\n",
    "                batch[1].to(device),\n",
    "                batch[2].to(device),\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Apply class weights to the loss\n",
    "                loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "                loss = loss_fct(outputs.logits, batch_labels)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                # Predictions for accuracy\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                eval_accuracy += accuracy_score(batch_labels.cpu(), preds.cpu())\n",
    "\n",
    "        # Average validation loss and accuracy\n",
    "        avg_val_loss = eval_loss / len(val_dataloader)\n",
    "        avg_val_accuracy = eval_accuracy / len(val_dataloader)\n",
    "        xm.master_print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "# Run training and get the trained model\n",
    "trained_model = train_tpu()\n",
    "trained_model.to(\"cpu\")\n",
    "\n",
    "# After training is finished, save the model\n",
    "print(\"Saving the fine-tuned model...\")\n",
    "trained_model.save_pretrained('./fake_news_model')\n",
    "tokenizer.save_pretrained('./fake_news_tokenizer')\n",
    "print(\"Model and Tokenizer saved to './fake_news_model'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T05:16:01.216094Z",
     "iopub.status.busy": "2024-11-14T05:16:01.215669Z",
     "iopub.status.idle": "2024-11-14T05:16:18.037043Z",
     "shell.execute_reply": "2024-11-14T05:16:18.035941Z",
     "shell.execute_reply.started": "2024-11-14T05:16:01.216059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4400\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.60      0.10      0.18        29\n",
      "     Class 1       0.42      0.90      0.58        21\n",
      "\n",
      "    accuracy                           0.44        50\n",
      "   macro avg       0.51      0.50      0.38        50\n",
      "weighted avg       0.53      0.44      0.34        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load test set\n",
    "test_df = pd.read_csv('/kaggle/input/test-set/train.csv')\n",
    "\n",
    "# Extract the top 50 rows\n",
    "test_df = test_df.head(50)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('./fake_news_tokenizer')\n",
    "model = BertForSequenceClassification.from_pretrained('./fake_news_model')\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the test data\n",
    "inputs = tokenizer(list(test_df['text']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_df['label'], predictions)\n",
    "report = classification_report(test_df['label'], predictions, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extend the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:26:29.082641Z",
     "iopub.status.busy": "2024-11-15T13:26:29.082272Z",
     "iopub.status.idle": "2024-11-15T13:26:33.446315Z",
     "shell.execute_reply": "2024-11-15T13:26:33.445333Z",
     "shell.execute_reply.started": "2024-11-15T13:26:29.082609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fake = pd.read_csv('/kaggle/input/new-fake-news/Fake.csv')\n",
    "real = pd.read_csv('/kaggle/input/new-fake-news/True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:26:48.287151Z",
     "iopub.status.busy": "2024-11-15T13:26:48.286687Z",
     "iopub.status.idle": "2024-11-15T13:26:48.464865Z",
     "shell.execute_reply": "2024-11-15T13:26:48.464134Z",
     "shell.execute_reply.started": "2024-11-15T13:26:48.287117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fake_df = pd.DataFrame(fake['title'] + fake['text'], columns=['title'])\n",
    "real_df = pd.DataFrame(real['title'] + real['text'], columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:26:55.659302Z",
     "iopub.status.busy": "2024-11-15T13:26:55.658578Z",
     "iopub.status.idle": "2024-11-15T13:26:55.674394Z",
     "shell.execute_reply": "2024-11-15T13:26:55.673696Z",
     "shell.execute_reply.started": "2024-11-15T13:26:55.659265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23481 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...\n",
       "1       Drunk Bragging Trump Staffer Started Russian ...\n",
       "2       Sheriff David Clarke Becomes An Internet Joke...\n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...\n",
       "4       Pope Francis Just Called Out Donald Trump Dur...\n",
       "...                                                  ...\n",
       "23476  McPain: John McCain Furious That Iran Treated ...\n",
       "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...\n",
       "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...\n",
       "23479  How to Blow $700 Million: Al Jazeera America F...\n",
       "23480  10 U.S. Navy Sailors Held by Iranian Military ...\n",
       "\n",
       "[23481 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:26:57.027193Z",
     "iopub.status.busy": "2024-11-15T13:26:57.026540Z",
     "iopub.status.idle": "2024-11-15T13:26:57.031762Z",
     "shell.execute_reply": "2024-11-15T13:26:57.031106Z",
     "shell.execute_reply.started": "2024-11-15T13:26:57.027160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fake_df['label']=1\n",
    "real_df['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:26:58.030972Z",
     "iopub.status.busy": "2024-11-15T13:26:58.030586Z",
     "iopub.status.idle": "2024-11-15T13:26:58.040741Z",
     "shell.execute_reply": "2024-11-15T13:26:58.039980Z",
     "shell.execute_reply.started": "2024-11-15T13:26:58.030940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "news_df_2 = pd.concat([fake_df, real_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:26:58.766674Z",
     "iopub.status.busy": "2024-11-15T13:26:58.765987Z",
     "iopub.status.idle": "2024-11-15T13:26:58.773676Z",
     "shell.execute_reply": "2024-11-15T13:26:58.773060Z",
     "shell.execute_reply.started": "2024-11-15T13:26:58.766644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  label\n",
       "44893  'Fully committed' NATO backs new U.S. approach...      0\n",
       "44894  LexisNexis withdrew two products from Chinese ...      0\n",
       "44895  Minsk cultural hub becomes haven from authorit...      0\n",
       "44896  Vatican upbeat on possibility of Pope Francis ...      0\n",
       "44897  Indonesia to buy $1.14 billion worth of Russia...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:29:13.925290Z",
     "iopub.status.busy": "2024-11-15T13:29:13.924144Z",
     "iopub.status.idle": "2024-11-15T13:29:22.403395Z",
     "shell.execute_reply": "2024-11-15T13:29:22.402321Z",
     "shell.execute_reply.started": "2024-11-15T13:29:13.925245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove special characters and digits, and convert text to lowercase\n",
    "\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "news_df_2['title'] = news_df_2['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:29:28.081528Z",
     "iopub.status.busy": "2024-11-15T13:29:28.081124Z",
     "iopub.status.idle": "2024-11-15T13:42:44.986156Z",
     "shell.execute_reply": "2024-11-15T13:42:44.985103Z",
     "shell.execute_reply.started": "2024-11-15T13:29:28.081496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "news_df_2['input_ids'] = news_df_2['title'].apply(lambda x: tokenizer(x, padding='max_length', truncation=True, max_length=512)['input_ids'])\n",
    "\n",
    "news_df_2['attention_mask'] = news_df_2['title'].apply(lambda x: tokenizer(x, padding='max_length', truncation=True, max_length=512)['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:43:45.487648Z",
     "iopub.status.busy": "2024-11-15T13:43:45.487204Z",
     "iopub.status.idle": "2024-11-15T13:43:45.493661Z",
     "shell.execute_reply": "2024-11-15T13:43:45.492879Z",
     "shell.execute_reply.started": "2024-11-15T13:43:45.487615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:43:48.277106Z",
     "iopub.status.busy": "2024-11-15T13:43:48.276052Z",
     "iopub.status.idle": "2024-11-15T13:43:48.284113Z",
     "shell.execute_reply": "2024-11-15T13:43:48.283327Z",
     "shell.execute_reply.started": "2024-11-15T13:43:48.277066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training data:\n",
      "label\n",
      "1    23481\n",
      "0    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataset is in a DataFrame named 'train_data' and the label column is named 'label'\n",
    "label_counts = news_df_2['label'].value_counts()\n",
    "print(\"Class distribution in training data:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:44:04.189433Z",
     "iopub.status.busy": "2024-11-15T13:44:04.188307Z",
     "iopub.status.idle": "2024-11-15T13:44:06.477937Z",
     "shell.execute_reply": "2024-11-15T13:44:06.476990Z",
     "shell.execute_reply.started": "2024-11-15T13:44:04.189397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(news_df_2[['input_ids', 'attention_mask']], news_df_2['label'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:44:20.650369Z",
     "iopub.status.busy": "2024-11-15T13:44:20.649046Z",
     "iopub.status.idle": "2024-11-15T13:44:29.366581Z",
     "shell.execute_reply": "2024-11-15T13:44:29.365435Z",
     "shell.execute_reply.started": "2024-11-15T13:44:20.650325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "\n",
    "train_inputs = torch.tensor([x for x in train_texts['input_ids']])\n",
    "\n",
    "train_masks = torch.tensor([x for x in train_texts['attention_mask']])\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values)\n",
    "\n",
    "\n",
    "\n",
    "val_inputs = torch.tensor([x for x in val_texts['input_ids']])\n",
    "\n",
    "val_masks = torch.tensor([x for x in val_texts['attention_mask']])\n",
    "\n",
    "val_labels = torch.tensor(val_labels.values)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T13:44:39.683196Z",
     "iopub.status.busy": "2024-11-15T13:44:39.682240Z",
     "iopub.status.idle": "2024-11-15T16:16:52.218808Z",
     "shell.execute_reply": "2024-11-15T16:16:52.217741Z",
     "shell.execute_reply.started": "2024-11-15T13:44:39.683150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1731678281.545088      13 common_lib.cc:818] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/common_lib.cc:483\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_13/3885354821.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Step [0/10102], Avg Loss: 0.1811\n",
      "Step [100/10102], Avg Loss: 0.1756\n",
      "Step [200/10102], Avg Loss: 0.1758\n",
      "Step [300/10102], Avg Loss: 0.1755\n",
      "Step [400/10102], Avg Loss: 0.1757\n",
      "Step [500/10102], Avg Loss: 0.1758\n",
      "Step [600/10102], Avg Loss: 0.1754\n",
      "Step [700/10102], Avg Loss: 0.1751\n",
      "Step [800/10102], Avg Loss: 0.1751\n",
      "Step [900/10102], Avg Loss: 0.1752\n",
      "Step [1000/10102], Avg Loss: 0.1752\n",
      "Step [1100/10102], Avg Loss: 0.1751\n",
      "Step [1200/10102], Avg Loss: 0.1750\n",
      "Step [1300/10102], Avg Loss: 0.1750\n",
      "Step [1400/10102], Avg Loss: 0.1750\n",
      "Step [1500/10102], Avg Loss: 0.1749\n",
      "Step [1600/10102], Avg Loss: 0.1748\n",
      "Step [1700/10102], Avg Loss: 0.1748\n",
      "Step [1800/10102], Avg Loss: 0.1749\n",
      "Step [1900/10102], Avg Loss: 0.1747\n",
      "Step [2000/10102], Avg Loss: 0.1746\n",
      "Step [2100/10102], Avg Loss: 0.1746\n",
      "Step [2200/10102], Avg Loss: 0.1747\n",
      "Step [2300/10102], Avg Loss: 0.1746\n",
      "Step [2400/10102], Avg Loss: 0.1746\n",
      "Step [2500/10102], Avg Loss: 0.1746\n",
      "Step [2600/10102], Avg Loss: 0.1745\n",
      "Step [2700/10102], Avg Loss: 0.1744\n",
      "Step [2800/10102], Avg Loss: 0.1744\n",
      "Step [2900/10102], Avg Loss: 0.1744\n",
      "Step [3000/10102], Avg Loss: 0.1743\n",
      "Step [3100/10102], Avg Loss: 0.1744\n",
      "Step [3200/10102], Avg Loss: 0.1743\n",
      "Step [3300/10102], Avg Loss: 0.1743\n",
      "Step [3400/10102], Avg Loss: 0.1743\n",
      "Step [3500/10102], Avg Loss: 0.1742\n",
      "Step [3600/10102], Avg Loss: 0.1742\n",
      "Step [3700/10102], Avg Loss: 0.1742\n",
      "Step [3800/10102], Avg Loss: 0.1741\n",
      "Step [3900/10102], Avg Loss: 0.1741\n",
      "Step [4000/10102], Avg Loss: 0.1741\n",
      "Step [4100/10102], Avg Loss: 0.1741\n",
      "Step [4200/10102], Avg Loss: 0.1740\n",
      "Step [4300/10102], Avg Loss: 0.1740\n",
      "Step [4400/10102], Avg Loss: 0.1739\n",
      "Step [4500/10102], Avg Loss: 0.1738\n",
      "Step [4600/10102], Avg Loss: 0.1738\n",
      "Step [4700/10102], Avg Loss: 0.1737\n",
      "Step [4800/10102], Avg Loss: 0.1737\n",
      "Step [4900/10102], Avg Loss: 0.1736\n",
      "Step [5000/10102], Avg Loss: 0.1736\n",
      "Step [5100/10102], Avg Loss: 0.1735\n",
      "Step [5200/10102], Avg Loss: 0.1735\n",
      "Step [5300/10102], Avg Loss: 0.1734\n",
      "Step [5400/10102], Avg Loss: 0.1733\n",
      "Step [5500/10102], Avg Loss: 0.1732\n",
      "Step [5600/10102], Avg Loss: 0.1732\n",
      "Step [5700/10102], Avg Loss: 0.1731\n",
      "Step [5800/10102], Avg Loss: 0.1730\n",
      "Step [5900/10102], Avg Loss: 0.1730\n",
      "Step [6000/10102], Avg Loss: 0.1729\n",
      "Step [6100/10102], Avg Loss: 0.1728\n",
      "Step [6200/10102], Avg Loss: 0.1728\n",
      "Step [6300/10102], Avg Loss: 0.1727\n",
      "Step [6400/10102], Avg Loss: 0.1726\n",
      "Step [6500/10102], Avg Loss: 0.1725\n",
      "Step [6600/10102], Avg Loss: 0.1724\n",
      "Step [6700/10102], Avg Loss: 0.1723\n",
      "Step [6800/10102], Avg Loss: 0.1722\n",
      "Step [6900/10102], Avg Loss: 0.1721\n",
      "Step [7000/10102], Avg Loss: 0.1720\n",
      "Step [7100/10102], Avg Loss: 0.1719\n",
      "Step [7200/10102], Avg Loss: 0.1718\n",
      "Step [7300/10102], Avg Loss: 0.1717\n",
      "Step [7400/10102], Avg Loss: 0.1716\n",
      "Step [7500/10102], Avg Loss: 0.1715\n",
      "Step [7600/10102], Avg Loss: 0.1714\n",
      "Step [7700/10102], Avg Loss: 0.1713\n",
      "Step [7800/10102], Avg Loss: 0.1712\n",
      "Step [7900/10102], Avg Loss: 0.1711\n",
      "Step [8000/10102], Avg Loss: 0.1709\n",
      "Step [8100/10102], Avg Loss: 0.1707\n",
      "Step [8200/10102], Avg Loss: 0.1706\n",
      "Step [8300/10102], Avg Loss: 0.1705\n",
      "Step [8400/10102], Avg Loss: 0.1703\n",
      "Step [8500/10102], Avg Loss: 0.1702\n",
      "Step [8600/10102], Avg Loss: 0.1700\n",
      "Step [8700/10102], Avg Loss: 0.1699\n",
      "Step [8800/10102], Avg Loss: 0.1697\n",
      "Step [8900/10102], Avg Loss: 0.1696\n",
      "Step [9000/10102], Avg Loss: 0.1695\n",
      "Step [9100/10102], Avg Loss: 0.1693\n",
      "Step [9200/10102], Avg Loss: 0.1691\n",
      "Step [9300/10102], Avg Loss: 0.1689\n",
      "Step [9400/10102], Avg Loss: 0.1688\n",
      "Step [9500/10102], Avg Loss: 0.1686\n",
      "Step [9600/10102], Avg Loss: 0.1684\n",
      "Step [9700/10102], Avg Loss: 0.1682\n",
      "Step [9800/10102], Avg Loss: 0.1681\n",
      "Step [9900/10102], Avg Loss: 0.1679\n",
      "Step [10000/10102], Avg Loss: 0.1678\n",
      "Step [10100/10102], Avg Loss: 0.1676\n",
      "Epoch [1/3] Training Loss: 0.6703\n",
      "Validation loss: 0.5803, Validation accuracy: 0.7576\n",
      "Epoch 2/3\n",
      "Step [0/10102], Avg Loss: 0.1256\n",
      "Step [100/10102], Avg Loss: 0.1458\n",
      "Step [200/10102], Avg Loss: 0.1484\n",
      "Step [300/10102], Avg Loss: 0.1478\n",
      "Step [400/10102], Avg Loss: 0.1472\n",
      "Step [500/10102], Avg Loss: 0.1465\n",
      "Step [600/10102], Avg Loss: 0.1460\n",
      "Step [700/10102], Avg Loss: 0.1457\n",
      "Step [800/10102], Avg Loss: 0.1454\n",
      "Step [900/10102], Avg Loss: 0.1453\n",
      "Step [1000/10102], Avg Loss: 0.1450\n",
      "Step [1100/10102], Avg Loss: 0.1447\n",
      "Step [1200/10102], Avg Loss: 0.1439\n",
      "Step [1300/10102], Avg Loss: 0.1436\n",
      "Step [1400/10102], Avg Loss: 0.1430\n",
      "Step [1500/10102], Avg Loss: 0.1423\n",
      "Step [1600/10102], Avg Loss: 0.1419\n",
      "Step [1700/10102], Avg Loss: 0.1413\n",
      "Step [1800/10102], Avg Loss: 0.1407\n",
      "Step [1900/10102], Avg Loss: 0.1400\n",
      "Step [2000/10102], Avg Loss: 0.1393\n",
      "Step [2100/10102], Avg Loss: 0.1388\n",
      "Step [2200/10102], Avg Loss: 0.1385\n",
      "Step [2300/10102], Avg Loss: 0.1380\n",
      "Step [2400/10102], Avg Loss: 0.1376\n",
      "Step [2500/10102], Avg Loss: 0.1370\n",
      "Step [2600/10102], Avg Loss: 0.1366\n",
      "Step [2700/10102], Avg Loss: 0.1359\n",
      "Step [2800/10102], Avg Loss: 0.1355\n",
      "Step [2900/10102], Avg Loss: 0.1349\n",
      "Step [3000/10102], Avg Loss: 0.1344\n",
      "Step [3100/10102], Avg Loss: 0.1340\n",
      "Step [3200/10102], Avg Loss: 0.1335\n",
      "Step [3300/10102], Avg Loss: 0.1331\n",
      "Step [3400/10102], Avg Loss: 0.1327\n",
      "Step [3500/10102], Avg Loss: 0.1322\n",
      "Step [3600/10102], Avg Loss: 0.1316\n",
      "Step [3700/10102], Avg Loss: 0.1313\n",
      "Step [3800/10102], Avg Loss: 0.1308\n",
      "Step [3900/10102], Avg Loss: 0.1302\n",
      "Step [4000/10102], Avg Loss: 0.1297\n",
      "Step [4100/10102], Avg Loss: 0.1292\n",
      "Step [4200/10102], Avg Loss: 0.1287\n",
      "Step [4300/10102], Avg Loss: 0.1283\n",
      "Step [4400/10102], Avg Loss: 0.1278\n",
      "Step [4500/10102], Avg Loss: 0.1273\n",
      "Step [4600/10102], Avg Loss: 0.1269\n",
      "Step [4700/10102], Avg Loss: 0.1263\n",
      "Step [4800/10102], Avg Loss: 0.1259\n",
      "Step [4900/10102], Avg Loss: 0.1254\n",
      "Step [5000/10102], Avg Loss: 0.1248\n",
      "Step [5100/10102], Avg Loss: 0.1243\n",
      "Step [5200/10102], Avg Loss: 0.1238\n",
      "Step [5300/10102], Avg Loss: 0.1232\n",
      "Step [5400/10102], Avg Loss: 0.1227\n",
      "Step [5500/10102], Avg Loss: 0.1223\n",
      "Step [5600/10102], Avg Loss: 0.1219\n",
      "Step [5700/10102], Avg Loss: 0.1215\n",
      "Step [5800/10102], Avg Loss: 0.1211\n",
      "Step [5900/10102], Avg Loss: 0.1209\n",
      "Step [6000/10102], Avg Loss: 0.1204\n",
      "Step [6100/10102], Avg Loss: 0.1200\n",
      "Step [6200/10102], Avg Loss: 0.1196\n",
      "Step [6300/10102], Avg Loss: 0.1191\n",
      "Step [6400/10102], Avg Loss: 0.1187\n",
      "Step [6500/10102], Avg Loss: 0.1184\n",
      "Step [6600/10102], Avg Loss: 0.1181\n",
      "Step [6700/10102], Avg Loss: 0.1176\n",
      "Step [6800/10102], Avg Loss: 0.1173\n",
      "Step [6900/10102], Avg Loss: 0.1168\n",
      "Step [7000/10102], Avg Loss: 0.1165\n",
      "Step [7100/10102], Avg Loss: 0.1161\n",
      "Step [7200/10102], Avg Loss: 0.1156\n",
      "Step [7300/10102], Avg Loss: 0.1155\n",
      "Step [7400/10102], Avg Loss: 0.1153\n",
      "Step [7500/10102], Avg Loss: 0.1150\n",
      "Step [7600/10102], Avg Loss: 0.1147\n",
      "Step [7700/10102], Avg Loss: 0.1144\n",
      "Step [7800/10102], Avg Loss: 0.1141\n",
      "Step [7900/10102], Avg Loss: 0.1139\n",
      "Step [8000/10102], Avg Loss: 0.1135\n",
      "Step [8100/10102], Avg Loss: 0.1132\n",
      "Step [8200/10102], Avg Loss: 0.1128\n",
      "Step [8300/10102], Avg Loss: 0.1126\n",
      "Step [8400/10102], Avg Loss: 0.1123\n",
      "Step [8500/10102], Avg Loss: 0.1120\n",
      "Step [8600/10102], Avg Loss: 0.1118\n",
      "Step [8700/10102], Avg Loss: 0.1116\n",
      "Step [8800/10102], Avg Loss: 0.1112\n",
      "Step [8900/10102], Avg Loss: 0.1110\n",
      "Step [9000/10102], Avg Loss: 0.1106\n",
      "Step [9100/10102], Avg Loss: 0.1104\n",
      "Step [9200/10102], Avg Loss: 0.1101\n",
      "Step [9300/10102], Avg Loss: 0.1101\n",
      "Step [9400/10102], Avg Loss: 0.1098\n",
      "Step [9500/10102], Avg Loss: 0.1095\n",
      "Step [9600/10102], Avg Loss: 0.1092\n",
      "Step [9700/10102], Avg Loss: 0.1089\n",
      "Step [9800/10102], Avg Loss: 0.1086\n",
      "Step [9900/10102], Avg Loss: 0.1085\n",
      "Step [10000/10102], Avg Loss: 0.1082\n",
      "Step [10100/10102], Avg Loss: 0.1079\n",
      "Epoch [2/3] Training Loss: 0.4315\n",
      "Validation loss: 0.3270, Validation accuracy: 0.8709\n",
      "Epoch 3/3\n",
      "Step [0/10102], Avg Loss: 0.1482\n",
      "Step [100/10102], Avg Loss: 0.0801\n",
      "Step [200/10102], Avg Loss: 0.0759\n",
      "Step [300/10102], Avg Loss: 0.0794\n",
      "Step [400/10102], Avg Loss: 0.0803\n",
      "Step [500/10102], Avg Loss: 0.0806\n",
      "Step [600/10102], Avg Loss: 0.0813\n",
      "Step [700/10102], Avg Loss: 0.0820\n",
      "Step [800/10102], Avg Loss: 0.0832\n",
      "Step [900/10102], Avg Loss: 0.0824\n",
      "Step [1000/10102], Avg Loss: 0.0818\n",
      "Step [1100/10102], Avg Loss: 0.0806\n",
      "Step [1200/10102], Avg Loss: 0.0809\n",
      "Step [1300/10102], Avg Loss: 0.0811\n",
      "Step [1400/10102], Avg Loss: 0.0812\n",
      "Step [1500/10102], Avg Loss: 0.0810\n",
      "Step [1600/10102], Avg Loss: 0.0811\n",
      "Step [1700/10102], Avg Loss: 0.0808\n",
      "Step [1800/10102], Avg Loss: 0.0806\n",
      "Step [1900/10102], Avg Loss: 0.0799\n",
      "Step [2000/10102], Avg Loss: 0.0798\n",
      "Step [2100/10102], Avg Loss: 0.0806\n",
      "Step [2200/10102], Avg Loss: 0.0804\n",
      "Step [2300/10102], Avg Loss: 0.0809\n",
      "Step [2400/10102], Avg Loss: 0.0805\n",
      "Step [2500/10102], Avg Loss: 0.0806\n",
      "Step [2600/10102], Avg Loss: 0.0803\n",
      "Step [2700/10102], Avg Loss: 0.0801\n",
      "Step [2800/10102], Avg Loss: 0.0802\n",
      "Step [2900/10102], Avg Loss: 0.0801\n",
      "Step [3000/10102], Avg Loss: 0.0799\n",
      "Step [3100/10102], Avg Loss: 0.0799\n",
      "Step [3200/10102], Avg Loss: 0.0795\n",
      "Step [3300/10102], Avg Loss: 0.0791\n",
      "Step [3400/10102], Avg Loss: 0.0793\n",
      "Step [3500/10102], Avg Loss: 0.0790\n",
      "Step [3600/10102], Avg Loss: 0.0787\n",
      "Step [3700/10102], Avg Loss: 0.0789\n",
      "Step [3800/10102], Avg Loss: 0.0787\n",
      "Step [3900/10102], Avg Loss: 0.0787\n",
      "Step [4000/10102], Avg Loss: 0.0785\n",
      "Step [4100/10102], Avg Loss: 0.0785\n",
      "Step [4200/10102], Avg Loss: 0.0785\n",
      "Step [4300/10102], Avg Loss: 0.0785\n",
      "Step [4400/10102], Avg Loss: 0.0784\n",
      "Step [4500/10102], Avg Loss: 0.0783\n",
      "Step [4600/10102], Avg Loss: 0.0782\n",
      "Step [4700/10102], Avg Loss: 0.0780\n",
      "Step [4800/10102], Avg Loss: 0.0778\n",
      "Step [4900/10102], Avg Loss: 0.0778\n",
      "Step [5000/10102], Avg Loss: 0.0778\n",
      "Step [5100/10102], Avg Loss: 0.0777\n",
      "Step [5200/10102], Avg Loss: 0.0777\n",
      "Step [5300/10102], Avg Loss: 0.0775\n",
      "Step [5400/10102], Avg Loss: 0.0776\n",
      "Step [5500/10102], Avg Loss: 0.0776\n",
      "Step [5600/10102], Avg Loss: 0.0774\n",
      "Step [5700/10102], Avg Loss: 0.0772\n",
      "Step [5800/10102], Avg Loss: 0.0773\n",
      "Step [5900/10102], Avg Loss: 0.0773\n",
      "Step [6000/10102], Avg Loss: 0.0771\n",
      "Step [6100/10102], Avg Loss: 0.0772\n",
      "Step [6200/10102], Avg Loss: 0.0771\n",
      "Step [6300/10102], Avg Loss: 0.0771\n",
      "Step [6400/10102], Avg Loss: 0.0769\n",
      "Step [6500/10102], Avg Loss: 0.0770\n",
      "Step [6600/10102], Avg Loss: 0.0771\n",
      "Step [6700/10102], Avg Loss: 0.0769\n",
      "Step [6800/10102], Avg Loss: 0.0770\n",
      "Step [6900/10102], Avg Loss: 0.0769\n",
      "Step [7000/10102], Avg Loss: 0.0769\n",
      "Step [7100/10102], Avg Loss: 0.0769\n",
      "Step [7200/10102], Avg Loss: 0.0769\n",
      "Step [7300/10102], Avg Loss: 0.0769\n",
      "Step [7400/10102], Avg Loss: 0.0769\n",
      "Step [7500/10102], Avg Loss: 0.0769\n",
      "Step [7600/10102], Avg Loss: 0.0767\n",
      "Step [7700/10102], Avg Loss: 0.0766\n",
      "Step [7800/10102], Avg Loss: 0.0766\n",
      "Step [7900/10102], Avg Loss: 0.0764\n",
      "Step [8000/10102], Avg Loss: 0.0765\n",
      "Step [8100/10102], Avg Loss: 0.0765\n",
      "Step [8200/10102], Avg Loss: 0.0765\n",
      "Step [8300/10102], Avg Loss: 0.0764\n",
      "Step [8400/10102], Avg Loss: 0.0763\n",
      "Step [8500/10102], Avg Loss: 0.0762\n",
      "Step [8600/10102], Avg Loss: 0.0762\n",
      "Step [8700/10102], Avg Loss: 0.0760\n",
      "Step [8800/10102], Avg Loss: 0.0760\n",
      "Step [8900/10102], Avg Loss: 0.0758\n",
      "Step [9000/10102], Avg Loss: 0.0758\n",
      "Step [9100/10102], Avg Loss: 0.0757\n",
      "Step [9200/10102], Avg Loss: 0.0756\n",
      "Step [9300/10102], Avg Loss: 0.0756\n",
      "Step [9400/10102], Avg Loss: 0.0756\n",
      "Step [9500/10102], Avg Loss: 0.0756\n",
      "Step [9600/10102], Avg Loss: 0.0755\n",
      "Step [9700/10102], Avg Loss: 0.0754\n",
      "Step [9800/10102], Avg Loss: 0.0754\n",
      "Step [9900/10102], Avg Loss: 0.0754\n",
      "Step [10000/10102], Avg Loss: 0.0754\n",
      "Step [10100/10102], Avg Loss: 0.0755\n",
      "Epoch [3/3] Training Loss: 0.3018\n",
      "Validation loss: 0.2992, Validation accuracy: 0.8845\n",
      "Saving the fine-tuned model...\n",
      "Model and Tokenizer saved to './fake_news_model'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.cuda.amp import GradScaler  # Only use GradScaler without autocast for TPU\n",
    "\n",
    "# Define the main training function\n",
    "def train_tpu():\n",
    "    # Setup device (TPU core)\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    # Load BERT model and move it to TPU device\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 2e-5\n",
    "    batch_size = 4\n",
    "    accumulation_steps = 4  # To simulate larger batch size\n",
    "    epochs = 3\n",
    "\n",
    "    # Optimizer and Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        para_loader = pl.ParallelLoader(train_dataloader, [device])\n",
    "\n",
    "        for step, batch in enumerate(para_loader.per_device_loader(device)):\n",
    "            batch_input_ids, batch_input_mask, batch_labels = (\n",
    "                batch[0].to(device),\n",
    "                batch[1].to(device),\n",
    "                batch[2].to(device),\n",
    "            )\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "            loss = outputs.loss / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Optimizer and Scheduler steps\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                xm.optimizer_step(optimizer, barrier=True)\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Log progress\n",
    "            if step % 100 == 0:\n",
    "                avg_loss = total_loss / ((step + 1) * batch_size)\n",
    "                print(f\"Step [{step}/{len(train_dataloader)}], Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_accuracy = 0\n",
    "        para_loader = pl.ParallelLoader(val_dataloader, [device])\n",
    "\n",
    "        for batch in para_loader.per_device_loader(device):\n",
    "            batch_input_ids, batch_input_mask, batch_labels = (\n",
    "                batch[0].to(device),\n",
    "                batch[1].to(device),\n",
    "                batch[2].to(device),\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                # Predictions for accuracy\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                eval_accuracy += accuracy_score(batch_labels.cpu(), preds.cpu())\n",
    "\n",
    "        avg_val_loss = eval_loss / len(val_dataloader)\n",
    "        avg_val_accuracy = eval_accuracy / len(val_dataloader)\n",
    "        xm.master_print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "# Run training and get the trained model\n",
    "trained_model = train_tpu()\n",
    "trained_model.to(\"cpu\")\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "print(\"Saving the fine-tuned model...\")\n",
    "trained_model.save_pretrained('./fake_news_model')\n",
    "tokenizer.save_pretrained('./fake_news_tokenizer')\n",
    "print(\"Model and Tokenizer saved to './fake_news_model'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:18:23.393333Z",
     "iopub.status.busy": "2024-11-15T16:18:23.391784Z",
     "iopub.status.idle": "2024-11-15T16:18:44.623557Z",
     "shell.execute_reply": "2024-11-15T16:18:44.622384Z",
     "shell.execute_reply.started": "2024-11-15T16:18:23.393256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer have been zipped.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip the model and tokenizer directories\n",
    "shutil.make_archive(\"/kaggle/working/fake_news_model\", 'zip', \"./fake_news_model\")\n",
    "shutil.make_archive(\"/kaggle/working/fake_news_tokenizer\", 'zip', \"./fake_news_tokenizer\")\n",
    "\n",
    "print(\"Model and tokenizer have been zipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:21:47.173532Z",
     "iopub.status.busy": "2024-11-15T16:21:47.173137Z",
     "iopub.status.idle": "2024-11-15T16:21:47.179372Z",
     "shell.execute_reply": "2024-11-15T16:21:47.178506Z",
     "shell.execute_reply.started": "2024-11-15T16:21:47.173501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='fake_news_model.zip' target='_blank'>fake_news_model.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/fake_news_model.zip"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'fake_news_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:19:11.472585Z",
     "iopub.status.busy": "2024-11-15T16:19:11.471578Z",
     "iopub.status.idle": "2024-11-15T16:19:12.232704Z",
     "shell.execute_reply": "2024-11-15T16:19:12.231614Z",
     "shell.execute_reply.started": "2024-11-15T16:19:11.472540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1945978785.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_label = pd.DataFrame(test['label'].replace({'FAKE': 1, 'REAL': 0}), columns=['label'])\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('/kaggle/input/test-set-3/news.csv')\n",
    "test_data = test['title'] + test['text']\n",
    "test_data = pd.DataFrame(test_data, columns=['text'])\n",
    "test_label = pd.DataFrame(test['label'].replace({'FAKE': 1, 'REAL': 0}), columns=['label'])\n",
    "test = pd.concat([test_data, test_label], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:29:32.465341Z",
     "iopub.status.busy": "2024-11-15T16:29:32.464880Z",
     "iopub.status.idle": "2024-11-15T16:29:32.475268Z",
     "shell.execute_reply": "2024-11-15T16:29:32.473990Z",
     "shell.execute_reply.started": "2024-11-15T16:29:32.465306Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did miley cyrus and liam hemsworth secretly ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paris jackson   cara delevingne enjoy night ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>celebrities join tax march in protest of donal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cindy crawford s daughter kaia gerber wears a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full list of      oscar nominations   variety</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>here s what really happened when jfk jr  met p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>biggest celebrity scandals of</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>caitlyn jenner addresses rumored romance with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>taylor swift reportedly reacts to tom hiddlest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>for the love of god  why can t anyone write ka...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0  did miley cyrus and liam hemsworth secretly ge...      1\n",
       "1  paris jackson   cara delevingne enjoy night ou...      1\n",
       "2  celebrities join tax march in protest of donal...      1\n",
       "3  cindy crawford s daughter kaia gerber wears a ...      1\n",
       "4      full list of      oscar nominations   variety      1\n",
       "5  here s what really happened when jfk jr  met p...      1\n",
       "6                 biggest celebrity scandals of           1\n",
       "7  caitlyn jenner addresses rumored romance with ...      1\n",
       "8  taylor swift reportedly reacts to tom hiddlest...      1\n",
       "9  for the love of god  why can t anyone write ka...      1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T16:33:42.360252Z",
     "iopub.status.busy": "2024-11-15T16:33:42.359333Z",
     "iopub.status.idle": "2024-11-15T16:33:44.973598Z",
     "shell.execute_reply": "2024-11-15T16:33:44.972492Z",
     "shell.execute_reply.started": "2024-11-15T16:33:42.360217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.83      1.00      0.91         5\n",
      "     Class 1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.92      0.90      0.90        10\n",
      "weighted avg       0.92      0.90      0.90        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load test set\n",
    "test_df = test.head(10)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('/kaggle/working/fake_news_tokenizer')\n",
    "model = BertForSequenceClassification.from_pretrained('/kaggle/working/fake_news_model')\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the test data\n",
    "inputs = tokenizer(list(test_df['text']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_df['label'], predictions)\n",
    "report = classification_report(test_df['label'], predictions, target_names=['Class 0', 'Class 1'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6074059,
     "sourceId": 9890376,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6081351,
     "sourceId": 9900033,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6081449,
     "sourceId": 9900174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6089790,
     "sourceId": 9911160,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6089970,
     "sourceId": 9911409,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6089992,
     "sourceId": 9911436,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6093384,
     "sourceId": 9915748,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6093580,
     "sourceId": 9915997,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30788,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
